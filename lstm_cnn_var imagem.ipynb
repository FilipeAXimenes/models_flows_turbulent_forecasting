{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de carregamento de dados: 0.11 segundos\n",
      "Número total de modos utilizados: 20\n",
      "Tempo de pré-processamento: 0.02 segundos\n",
      "Treinando um novo modelo...\n",
      "Epoch 1/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 106ms/step - loss: 0.1320 - val_loss: 0.1050\n",
      "Epoch 2/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 144ms/step - loss: 0.0914 - val_loss: 0.0996\n",
      "Epoch 3/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 79ms/step - loss: 0.0810 - val_loss: 0.0945\n",
      "Epoch 4/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 75ms/step - loss: 0.0744 - val_loss: 0.0890\n",
      "Epoch 5/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - loss: 0.0680 - val_loss: 0.0824\n",
      "Epoch 6/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 202ms/step - loss: 0.0633 - val_loss: 0.0760\n",
      "Epoch 7/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 118ms/step - loss: 0.0592 - val_loss: 0.0693\n",
      "Epoch 8/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 100ms/step - loss: 0.0555 - val_loss: 0.0631\n",
      "Epoch 9/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - loss: 0.0519 - val_loss: 0.0578\n",
      "Epoch 10/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - loss: 0.0490 - val_loss: 0.0533\n",
      "Epoch 11/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - loss: 0.0462 - val_loss: 0.0495\n",
      "Epoch 12/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 110ms/step - loss: 0.0438 - val_loss: 0.0468\n",
      "Epoch 13/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 91ms/step - loss: 0.0417 - val_loss: 0.0440\n",
      "Epoch 14/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 102ms/step - loss: 0.0401 - val_loss: 0.0412\n",
      "Epoch 15/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 98ms/step - loss: 0.0389 - val_loss: 0.0394\n",
      "Epoch 16/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 104ms/step - loss: 0.0375 - val_loss: 0.0372\n",
      "Epoch 17/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 104ms/step - loss: 0.0358 - val_loss: 0.0354\n",
      "Epoch 18/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 126ms/step - loss: 0.0351 - val_loss: 0.0340\n",
      "Epoch 19/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 108ms/step - loss: 0.0343 - val_loss: 0.0326\n",
      "Epoch 20/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 183ms/step - loss: 0.0330 - val_loss: 0.0316\n",
      "Epoch 21/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 166ms/step - loss: 0.0325 - val_loss: 0.0304\n",
      "Epoch 22/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 189ms/step - loss: 0.0319 - val_loss: 0.0295\n",
      "Epoch 23/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 188ms/step - loss: 0.0313 - val_loss: 0.0289\n",
      "Epoch 24/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 184ms/step - loss: 0.0306 - val_loss: 0.0277\n",
      "Epoch 25/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 177ms/step - loss: 0.0299 - val_loss: 0.0270\n",
      "Epoch 26/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 0.0296 - val_loss: 0.0262\n",
      "Epoch 27/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 163ms/step - loss: 0.0294 - val_loss: 0.0260\n",
      "Epoch 28/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 165ms/step - loss: 0.0288 - val_loss: 0.0252\n",
      "Epoch 29/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 144ms/step - loss: 0.0288 - val_loss: 0.0247\n",
      "Epoch 30/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 144ms/step - loss: 0.0279 - val_loss: 0.0240\n",
      "Epoch 31/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 140ms/step - loss: 0.0277 - val_loss: 0.0236\n",
      "Epoch 32/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 164ms/step - loss: 0.0278 - val_loss: 0.0233\n",
      "Epoch 33/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 149ms/step - loss: 0.0277 - val_loss: 0.0230\n",
      "Epoch 34/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 119ms/step - loss: 0.0273 - val_loss: 0.0225\n",
      "Epoch 35/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 125ms/step - loss: 0.0272 - val_loss: 0.0224\n",
      "Epoch 36/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 120ms/step - loss: 0.0269 - val_loss: 0.0219\n",
      "Epoch 37/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 110ms/step - loss: 0.0264 - val_loss: 0.0219\n",
      "Epoch 38/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 109ms/step - loss: 0.0268 - val_loss: 0.0215\n",
      "Epoch 39/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 135ms/step - loss: 0.0265 - val_loss: 0.0211\n",
      "Epoch 40/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 125ms/step - loss: 0.0264 - val_loss: 0.0210\n",
      "Epoch 41/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 117ms/step - loss: 0.0259 - val_loss: 0.0206\n",
      "Epoch 42/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 122ms/step - loss: 0.0260 - val_loss: 0.0207\n",
      "Epoch 43/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 154ms/step - loss: 0.0259 - val_loss: 0.0206\n",
      "Epoch 44/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 321ms/step - loss: 0.0255 - val_loss: 0.0203\n",
      "Epoch 45/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 169ms/step - loss: 0.0255 - val_loss: 0.0203\n",
      "Epoch 46/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 120ms/step - loss: 0.0256 - val_loss: 0.0200\n",
      "Epoch 47/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 171ms/step - loss: 0.0251 - val_loss: 0.0198\n",
      "Epoch 48/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 273ms/step - loss: 0.0253 - val_loss: 0.0197\n",
      "Epoch 49/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 168ms/step - loss: 0.0248 - val_loss: 0.0194\n",
      "Epoch 50/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 150ms/step - loss: 0.0248 - val_loss: 0.0194\n",
      "Epoch 51/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 212ms/step - loss: 0.0250 - val_loss: 0.0190\n",
      "Epoch 52/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 171ms/step - loss: 0.0250 - val_loss: 0.0191\n",
      "Epoch 53/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 156ms/step - loss: 0.0250 - val_loss: 0.0190\n",
      "Epoch 54/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 210ms/step - loss: 0.0239 - val_loss: 0.0190\n",
      "Epoch 55/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 158ms/step - loss: 0.0245 - val_loss: 0.0187\n",
      "Epoch 56/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 190ms/step - loss: 0.0242 - val_loss: 0.0185\n",
      "Epoch 57/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 166ms/step - loss: 0.0244 - val_loss: 0.0184\n",
      "Epoch 58/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 181ms/step - loss: 0.0244 - val_loss: 0.0183\n",
      "Epoch 59/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 164ms/step - loss: 0.0241 - val_loss: 0.0183\n",
      "Epoch 60/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 131ms/step - loss: 0.0236 - val_loss: 0.0182\n",
      "Epoch 61/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 131ms/step - loss: 0.0239 - val_loss: 0.0183\n",
      "Epoch 62/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 130ms/step - loss: 0.0248 - val_loss: 0.0183\n",
      "Epoch 63/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 127ms/step - loss: 0.0239 - val_loss: 0.0180\n",
      "Epoch 64/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 135ms/step - loss: 0.0235 - val_loss: 0.0179\n",
      "Epoch 65/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 135ms/step - loss: 0.0236 - val_loss: 0.0177\n",
      "Epoch 66/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 139ms/step - loss: 0.0235 - val_loss: 0.0178\n",
      "Epoch 67/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 150ms/step - loss: 0.0234 - val_loss: 0.0178\n",
      "Epoch 68/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 141ms/step - loss: 0.0234 - val_loss: 0.0177\n",
      "Epoch 69/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 298ms/step - loss: 0.0234 - val_loss: 0.0178\n",
      "Epoch 70/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 129ms/step - loss: 0.0235 - val_loss: 0.0174\n",
      "Epoch 71/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 121ms/step - loss: 0.0231 - val_loss: 0.0173\n",
      "Epoch 72/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 146ms/step - loss: 0.0231 - val_loss: 0.0170\n",
      "Epoch 73/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 154ms/step - loss: 0.0234 - val_loss: 0.0173\n",
      "Epoch 74/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 252ms/step - loss: 0.0230 - val_loss: 0.0175\n",
      "Epoch 75/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 208ms/step - loss: 0.0235 - val_loss: 0.0175\n",
      "Epoch 76/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 150ms/step - loss: 0.0236 - val_loss: 0.0171\n",
      "Epoch 77/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 133ms/step - loss: 0.0236 - val_loss: 0.0171\n",
      "Epoch 78/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 130ms/step - loss: 0.0233 - val_loss: 0.0169\n",
      "Epoch 79/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 138ms/step - loss: 0.0227 - val_loss: 0.0169\n",
      "Epoch 80/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 147ms/step - loss: 0.0229 - val_loss: 0.0170\n",
      "Epoch 81/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 169ms/step - loss: 0.0226 - val_loss: 0.0167\n",
      "Epoch 82/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 201ms/step - loss: 0.0228 - val_loss: 0.0166\n",
      "Epoch 83/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 193ms/step - loss: 0.0224 - val_loss: 0.0169\n",
      "Epoch 84/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 170ms/step - loss: 0.0234 - val_loss: 0.0170\n",
      "Epoch 85/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 210ms/step - loss: 0.0224 - val_loss: 0.0165\n",
      "Epoch 86/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 161ms/step - loss: 0.0226 - val_loss: 0.0167\n",
      "Epoch 87/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 177ms/step - loss: 0.0225 - val_loss: 0.0165\n",
      "Epoch 88/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 182ms/step - loss: 0.0229 - val_loss: 0.0168\n",
      "Epoch 89/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 229ms/step - loss: 0.0229 - val_loss: 0.0164\n",
      "Epoch 90/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 171ms/step - loss: 0.0228 - val_loss: 0.0165\n",
      "Epoch 91/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 160ms/step - loss: 0.0225 - val_loss: 0.0166\n",
      "Epoch 92/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 152ms/step - loss: 0.0230 - val_loss: 0.0167\n",
      "Epoch 93/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 150ms/step - loss: 0.0231 - val_loss: 0.0166\n",
      "Epoch 94/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 135ms/step - loss: 0.0220 - val_loss: 0.0165\n",
      "Epoch 95/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 151ms/step - loss: 0.0232 - val_loss: 0.0164\n",
      "Epoch 96/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 205ms/step - loss: 0.0221 - val_loss: 0.0165\n",
      "Epoch 97/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 159ms/step - loss: 0.0216 - val_loss: 0.0164\n",
      "Epoch 98/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - loss: 0.0222 - val_loss: 0.0163\n",
      "Epoch 99/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 151ms/step - loss: 0.0221 - val_loss: 0.0162\n",
      "Epoch 100/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 230ms/step - loss: 0.0222 - val_loss: 0.0164\n",
      "Epoch 101/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 150ms/step - loss: 0.0224 - val_loss: 0.0161\n",
      "Epoch 102/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 141ms/step - loss: 0.0222 - val_loss: 0.0160\n",
      "Epoch 103/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 131ms/step - loss: 0.0224 - val_loss: 0.0162\n",
      "Epoch 104/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 124ms/step - loss: 0.0224 - val_loss: 0.0160\n",
      "Epoch 105/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 143ms/step - loss: 0.0222 - val_loss: 0.0161\n",
      "Epoch 106/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 137ms/step - loss: 0.0228 - val_loss: 0.0167\n",
      "Epoch 107/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 140ms/step - loss: 0.0224 - val_loss: 0.0161\n",
      "Epoch 108/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 171ms/step - loss: 0.0225 - val_loss: 0.0164\n",
      "Epoch 109/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 198ms/step - loss: 0.0221 - val_loss: 0.0161\n",
      "Epoch 110/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 147ms/step - loss: 0.0218 - val_loss: 0.0159\n",
      "Epoch 111/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 136ms/step - loss: 0.0220 - val_loss: 0.0160\n",
      "Epoch 112/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 314ms/step - loss: 0.0221 - val_loss: 0.0162\n",
      "Epoch 113/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 201ms/step - loss: 0.0220 - val_loss: 0.0163\n",
      "Epoch 114/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 226ms/step - loss: 0.0217 - val_loss: 0.0158\n",
      "Epoch 115/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 239ms/step - loss: 0.0220 - val_loss: 0.0161\n",
      "Epoch 116/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 240ms/step - loss: 0.0219 - val_loss: 0.0160\n",
      "Epoch 117/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 247ms/step - loss: 0.0217 - val_loss: 0.0159\n",
      "Epoch 118/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 246ms/step - loss: 0.0220 - val_loss: 0.0162\n",
      "Epoch 119/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 201ms/step - loss: 0.0219 - val_loss: 0.0158\n",
      "Epoch 120/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 168ms/step - loss: 0.0213 - val_loss: 0.0158\n",
      "Epoch 121/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 195ms/step - loss: 0.0217 - val_loss: 0.0157\n",
      "Epoch 122/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 198ms/step - loss: 0.0212 - val_loss: 0.0153\n",
      "Epoch 123/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 155ms/step - loss: 0.0214 - val_loss: 0.0158\n",
      "Epoch 124/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 145ms/step - loss: 0.0216 - val_loss: 0.0160\n",
      "Epoch 125/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 152ms/step - loss: 0.0225 - val_loss: 0.0158\n",
      "Epoch 126/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 203ms/step - loss: 0.0213 - val_loss: 0.0159\n",
      "Epoch 127/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 222ms/step - loss: 0.0223 - val_loss: 0.0159\n",
      "Epoch 128/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 186ms/step - loss: 0.0225 - val_loss: 0.0158\n",
      "Epoch 129/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 176ms/step - loss: 0.0221 - val_loss: 0.0155\n",
      "Epoch 130/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 173ms/step - loss: 0.0220 - val_loss: 0.0159\n",
      "Epoch 131/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 204ms/step - loss: 0.0214 - val_loss: 0.0158\n",
      "Epoch 132/200\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 224ms/step - loss: 0.0211 - val_loss: 0.0155\n",
      "Tempo de treinamento: 861.00 segundos\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step\n",
      "Ordem do modelo VAR nos resíduos selecionada (AIC): 6\n",
      "Métricas para a parte real (CNN+LSTM+VAR): MSE: 0.000000, MAE: 0.000045, R²: 0.994536\n",
      "Métricas para a parte imaginária (CNN+LSTM+VAR): MSE: 0.000000, MAE: 0.000044, R²: 0.994789\n",
      "Média do NMSE para a parte real (CNN+LSTM+VAR): 0.007477\n",
      "Média do NMSE para a parte imaginária (CNN+LSTM+VAR): 0.007457\n",
      "Média do NMSE para o módulo dos coeficientes (CNN+LSTM+VAR): 0.017232\n",
      "Tempo total de execução do script: 967.26 segundos\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, LSTM, Conv1D, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "# Configurações gerais\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "\n",
    "# Diretório para salvar as figuras\n",
    "output_dir = 'figuras_cnn_lstm_var'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Hiperparâmetros atualizados\n",
    "hyperparams = {\n",
    "    'model_path': 'modelo_cnn_lstm_var.keras',\n",
    "    'num_coefs': 20,\n",
    "    'sequence_length': 30,  # Ajustado\n",
    "    'lstm_units': [128],  # Ajustado\n",
    "    'conv_filters': [64, 32],  # Ajustado\n",
    "    'kernel_size': 3,\n",
    "    'dropout_rate': 0.2,  # Ajustado\n",
    "    'l2_regularization': 0.0001,  # Ajustado\n",
    "    'learning_rate': 0.001,  # Ajustado\n",
    "    'epochs': 200,  # Ajustado\n",
    "    'batch_size': 64,  # Ajustado\n",
    "    'early_stopping_patience': 10  # Ajustado\n",
    "}\n",
    "\n",
    "# Medir o tempo total do script\n",
    "total_start_time = time.time()\n",
    "\n",
    "# Medição do tempo de carregamento de dados\n",
    "start_time = time.time()\n",
    "\n",
    "# Carregar os arquivos .mat fornecidos\n",
    "file_path_temp_coefs = \"temp_coefs.mat\"\n",
    "file_path_spatial_modes = \"spatial_modes.mat\"\n",
    "file_path_mean_flow = \"mean_flow.mat\"\n",
    "file_path_parameters = \"parameters.mat\"\n",
    "\n",
    "# Caminho para salvar/carregar o modelo\n",
    "model_path = hyperparams['model_path']\n",
    "\n",
    "# Carregar os coeficientes temporais\n",
    "mat_data_temp = sio.loadmat(file_path_temp_coefs)\n",
    "ts = mat_data_temp['ts'].flatten()  # Vetor de tempo\n",
    "coefs = mat_data_temp['coefs']      # Matriz de coeficientes complexos\n",
    "\n",
    "# Carregar os modos espaciais\n",
    "mat_data_spatial = sio.loadmat(file_path_spatial_modes)\n",
    "phi = mat_data_spatial['phi']       # Matriz de modos espaciais\n",
    "\n",
    "# Carregar o vetor y e a média Xavg\n",
    "mat_data_mean_flow = sio.loadmat(file_path_mean_flow)\n",
    "y_positions = mat_data_mean_flow['y'].flatten()  # Vetor de posições espaciais y\n",
    "Xavg = mat_data_mean_flow['Xavg'].flatten()      # Média do fluxo\n",
    "\n",
    "# Carregar os modos usados no POD\n",
    "mat_data_parameters = sio.loadmat(file_path_parameters)\n",
    "nmodos_pod = mat_data_parameters['nmodes']       # Modos usados no POD\n",
    "\n",
    "print(f\"Tempo de carregamento de dados: {time.time() - start_time:.2f} segundos\")\n",
    "\n",
    "# Medição do tempo de pré-processamento\n",
    "start_time = time.time()\n",
    "\n",
    "# Definir o número de coeficientes a serem utilizados\n",
    "num_coefs = hyperparams['num_coefs']\n",
    "print(f\"Número total de modos utilizados: {num_coefs}\")\n",
    "\n",
    "# Separar os coeficientes em parte real e imaginária\n",
    "real_coefs = np.real(coefs[:num_coefs, :])\n",
    "imag_coefs = np.imag(coefs[:num_coefs, :])\n",
    "\n",
    "# Definir o limite do primeiro intervalo\n",
    "first_interval_end = 4801  # Posição correspondente ao tempo 1300\n",
    "\n",
    "# Subamostragem do primeiro intervalo para taxa de 0.5 (pegando a cada 2 pontos)\n",
    "real_coefs_train = real_coefs[:, :first_interval_end:2]\n",
    "imag_coefs_train = imag_coefs[:, :first_interval_end:2]\n",
    "\n",
    "# Segundo intervalo (já na taxa de 0.5)\n",
    "real_coefs_val = real_coefs[:, first_interval_end:]\n",
    "imag_coefs_val = imag_coefs[:, first_interval_end:]\n",
    "\n",
    "# Concatenar as partes real e imaginária para normalização\n",
    "coefs_train = np.vstack([real_coefs_train, imag_coefs_train])  # Dimensão (num_coefs*2, N_train)\n",
    "coefs_val = np.vstack([real_coefs_val, imag_coefs_val])        # Dimensão (num_coefs*2, N_val)\n",
    "\n",
    "# Normalizar os dados de treinamento individualmente para cada coeficiente usando MinMaxScaler\n",
    "num_total_coefs = num_coefs * 2\n",
    "scalers = []\n",
    "\n",
    "coefs_train_normalized = np.zeros_like(coefs_train)\n",
    "coefs_val_normalized = np.zeros_like(coefs_val)\n",
    "\n",
    "for i in range(num_total_coefs):\n",
    "    scaler_coef = MinMaxScaler(feature_range=(-1, 1))\n",
    "    coefs_train_normalized[i, :] = scaler_coef.fit_transform(coefs_train[i, :].reshape(-1, 1)).flatten()\n",
    "    coefs_val_normalized[i, :] = scaler_coef.transform(coefs_val[i, :].reshape(-1, 1)).flatten()\n",
    "    scalers.append(scaler_coef)\n",
    "\n",
    "print(f\"Tempo de pré-processamento: {time.time() - start_time:.2f} segundos\")\n",
    "\n",
    "# Separar novamente as partes real e imaginária normalizadas\n",
    "real_coefs_train_normalized = coefs_train_normalized[:num_coefs, :]\n",
    "imag_coefs_train_normalized = coefs_train_normalized[num_coefs:, :]\n",
    "real_coefs_val_normalized = coefs_val_normalized[:num_coefs, :]\n",
    "imag_coefs_val_normalized = coefs_val_normalized[num_coefs:, :]\n",
    "\n",
    "# Definir o comprimento da sequência para a LSTM\n",
    "sequence_length = hyperparams['sequence_length']\n",
    "\n",
    "# Função para criar sequências de entrada e saídas correspondentes (previsão de um passo à frente)\n",
    "def create_sequences(real_coefs_norm, imag_coefs_norm, seq_length):\n",
    "    X = []\n",
    "    y = []\n",
    "    num_samples = real_coefs_norm.shape[1] - seq_length\n",
    "    for i in range(num_samples):\n",
    "        # Sequências das partes real e imaginária\n",
    "        X_seq_real = real_coefs_norm[:, i:i+seq_length]\n",
    "        X_seq_imag = imag_coefs_norm[:, i:i+seq_length]\n",
    "        X_seq = np.vstack([X_seq_real, X_seq_imag])  # Dimensão (num_coefs*2, seq_length)\n",
    "        X.append(X_seq)\n",
    "        # Saídas: próximo coeficiente após a sequência\n",
    "        y_real = real_coefs_norm[:, i+seq_length]\n",
    "        y_imag = imag_coefs_norm[:, i+seq_length]\n",
    "        y_seq = np.hstack([y_real, y_imag])  # Dimensão (num_coefs*2,)\n",
    "        y.append(y_seq)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    # Ajustar as dimensões para [samples, timesteps, features]\n",
    "    X = X.transpose(0, 2, 1)  # [samples, timesteps, features]\n",
    "    return X, y\n",
    "\n",
    "# Preparar os dados de treinamento\n",
    "X_train, y_train = create_sequences(real_coefs_train_normalized, imag_coefs_train_normalized, sequence_length)\n",
    "\n",
    "# Preparar os dados de validação\n",
    "X_val, y_val = create_sequences(real_coefs_val_normalized, imag_coefs_val_normalized, sequence_length)\n",
    "\n",
    "# Medição do tempo de treinamento\n",
    "start_time = time.time()\n",
    "\n",
    "# Verificar se o modelo já foi salvo anteriormente\n",
    "retrain_model = True\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Carregando o modelo salvo...\")\n",
    "    model = load_model(model_path)\n",
    "    if model.input_shape[1:] != (X_train.shape[1], X_train.shape[2]):\n",
    "        print(\"O modelo salvo não é compatível com os dados atuais. Treinando um novo modelo.\")\n",
    "        os.remove(model_path)\n",
    "        retrain_model = True\n",
    "    else:\n",
    "        retrain_model = False\n",
    "else:\n",
    "    print(\"Treinando um novo modelo...\")\n",
    "    retrain_model = True\n",
    "\n",
    "if retrain_model:\n",
    "    # Definir o modelo CNN+LSTM com regularização\n",
    "    lstm_input = Input(shape=(X_train.shape[1], X_train.shape[2]))  # [timesteps, features]\n",
    "\n",
    "    x = lstm_input  # Forma: (batch_size, timesteps, features)\n",
    "\n",
    "    # Aplicar Conv1D diretamente sobre a dimensão temporal\n",
    "    x = Conv1D(filters=hyperparams['conv_filters'][0],\n",
    "               kernel_size=hyperparams['kernel_size'],\n",
    "               activation='relu', padding='same',\n",
    "               kernel_regularizer=l2(hyperparams['l2_regularization']))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv1D(filters=hyperparams['conv_filters'][1],\n",
    "               kernel_size=hyperparams['kernel_size'],\n",
    "               activation='relu', padding='same',\n",
    "               kernel_regularizer=l2(hyperparams['l2_regularization']))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(hyperparams['dropout_rate'])(x)\n",
    "\n",
    "    # Camada LSTM\n",
    "    x = LSTM(hyperparams['lstm_units'][0], return_sequences=False, activation='tanh',\n",
    "             kernel_regularizer=l2(hyperparams['l2_regularization']),\n",
    "             recurrent_dropout=hyperparams['dropout_rate'])(x)\n",
    "    x = Dropout(hyperparams['dropout_rate'])(x)\n",
    "\n",
    "    # Camada de saída\n",
    "    output = Dense(num_coefs * 2, activation='linear',\n",
    "                   kernel_regularizer=l2(hyperparams['l2_regularization']))(x)\n",
    "\n",
    "    # Definir e compilar o modelo\n",
    "    model = Model(inputs=lstm_input, outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=hyperparams['learning_rate']), loss='mean_squared_error')\n",
    "\n",
    "    # Definir callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=hyperparams['early_stopping_patience'],\n",
    "                                   restore_best_weights=True)\n",
    "    checkpoint = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True, mode='min')\n",
    "\n",
    "    # Treinar o modelo\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        epochs=hyperparams['epochs'],\n",
    "                        batch_size=hyperparams['batch_size'],\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        callbacks=[early_stopping, checkpoint],\n",
    "                        verbose=1)\n",
    "\n",
    "    # Salvar o modelo treinado\n",
    "    model.save(model_path)\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Tempo de treinamento: {training_time:.2f} segundos\")\n",
    "else:\n",
    "    print(\"Usando o modelo salvo para fazer previsões.\")\n",
    "    training_time = 0  # Se não treinou, tempo é zero\n",
    "\n",
    "# Fazer previsões no conjunto de validação com o modelo CNN+LSTM\n",
    "cnn_lstm_predictions_normalized = model.predict(X_val)\n",
    "\n",
    "# Reverter a normalização das previsões da CNN+LSTM\n",
    "cnn_lstm_predictions = np.zeros_like(cnn_lstm_predictions_normalized)\n",
    "for i in range(num_total_coefs):\n",
    "    scaler = scalers[i]\n",
    "    cnn_lstm_predictions[:, i] = scaler.inverse_transform(cnn_lstm_predictions_normalized[:, i].reshape(-1, 1)).flatten()\n",
    "\n",
    "# Reverter a normalização dos valores reais\n",
    "y_val_original = np.zeros_like(y_val)\n",
    "for i in range(num_total_coefs):\n",
    "    scaler = scalers[i]\n",
    "    y_val_original[:, i] = scaler.inverse_transform(y_val[:, i].reshape(-1, 1)).flatten()\n",
    "\n",
    "# Calcular os resíduos entre os valores reais e as previsões da CNN+LSTM\n",
    "residuals = y_val_original - cnn_lstm_predictions\n",
    "\n",
    "# Ajustar o modelo VAR nos resíduos\n",
    "model_var = VAR(residuals)\n",
    "# Selecionar automaticamente a ordem do modelo (número de defasagens) usando o critério AIC\n",
    "results_aic = model_var.select_order(maxlags=15)\n",
    "selected_lag = results_aic.aic\n",
    "print(f\"Ordem do modelo VAR nos resíduos selecionada (AIC): {selected_lag}\")\n",
    "model_var_fitted = model_var.fit(selected_lag)\n",
    "\n",
    "# Previsões do VAR nos resíduos\n",
    "var_residuals_pred = model_var_fitted.fittedvalues\n",
    "\n",
    "# Preencher o início com zeros para alinhamento\n",
    "var_residuals_pred = np.vstack([np.zeros((selected_lag, num_total_coefs)), var_residuals_pred])\n",
    "\n",
    "# Previsão final = Previsão CNN+LSTM + Previsão VAR nos resíduos\n",
    "final_predictions = cnn_lstm_predictions[selected_lag:] + var_residuals_pred[selected_lag:]\n",
    "\n",
    "# Ajustar os valores reais correspondentes\n",
    "actual_values = y_val_original[selected_lag:]\n",
    "\n",
    "# Garantir que as dimensões correspondam\n",
    "min_length = min(final_predictions.shape[0], actual_values.shape[0])\n",
    "final_predictions = final_predictions[:min_length]\n",
    "actual_values = actual_values[:min_length]\n",
    "\n",
    "# Separar as partes real e imaginária das previsões e valores reais\n",
    "pred_real = final_predictions[:, :num_coefs]\n",
    "pred_imag = final_predictions[:, num_coefs:]\n",
    "actual_real = actual_values[:, :num_coefs]\n",
    "actual_imag = actual_values[:, num_coefs:]\n",
    "\n",
    "# Reconstruir os coeficientes complexos preditos e reais\n",
    "predicted_coefs = pred_real + 1j * pred_imag\n",
    "actual_coefs = actual_real + 1j * actual_imag\n",
    "\n",
    "# Reconstruir X a partir dos coeficientes preditos e dos modos espaciais\n",
    "phi_reduced = phi[:, :num_coefs]  # Dimensão (387, num_coefs)\n",
    "Xavg = Xavg.flatten()  # Dimensão (387,)\n",
    "\n",
    "X_rec_list = []\n",
    "X_actual_list = []\n",
    "\n",
    "for i in range(predicted_coefs.shape[0]):\n",
    "    # Reconstrução predita\n",
    "    X_rec = Xavg + phi_reduced @ predicted_coefs[i]\n",
    "    X_rec_list.append(X_rec)\n",
    "    # Reconstrução real\n",
    "    X_actual = Xavg + phi_reduced @ actual_coefs[i]\n",
    "    X_actual_list.append(X_actual)\n",
    "\n",
    "# Converter as listas em arrays e transpor\n",
    "X_rec_array = np.array(X_rec_list).T        # Forma (387, num_samples)\n",
    "X_actual_array = np.array(X_actual_list).T  # Forma (387, num_samples)\n",
    "\n",
    "# Número de pontos em y\n",
    "ny = y_positions.shape[0]  # Deve ser 129\n",
    "\n",
    "# Plotar a comparação entre os perfis originais e reconstruídos de |u|\n",
    "step = 10  # Número de subplots (ajuste conforme necessário)\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "plt.clf()\n",
    "for i in range(step):\n",
    "    plt.subplot(4, 5, i + 1)\n",
    "    # Índice do tempo\n",
    "    idx = i * (X_rec_array.shape[1] // step)\n",
    "    plt.plot(np.abs(X_actual_array[:ny, idx]), y_positions, label='Original')\n",
    "    plt.plot(np.abs(X_rec_array[:ny, idx]), y_positions, label='Reconstruído', linestyle='--')\n",
    "    plt.xlabel('|u|')\n",
    "    plt.ylabel('y')\n",
    "    plt.title(f'Amostra {idx}')\n",
    "    plt.grid(True)\n",
    "    if i == 0:\n",
    "        plt.legend()\n",
    "plt.suptitle('Comparação de |u| Original e Reconstruído (Validação) - CNN+LSTM+VAR')\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.savefig(os.path.join(output_dir, 'comparacao_u_original_reconstruido_cnn_lstm_var.png'), dpi=400)\n",
    "plt.close()\n",
    "\n",
    "# Calcular as métricas de erro separadamente para as partes real e imaginária\n",
    "mse_real = mean_squared_error(actual_real.flatten(), pred_real.flatten())\n",
    "mae_real = mean_absolute_error(actual_real.flatten(), pred_real.flatten())\n",
    "r2_real = r2_score(actual_real.flatten(), pred_real.flatten())\n",
    "\n",
    "mse_imag = mean_squared_error(actual_imag.flatten(), pred_imag.flatten())\n",
    "mae_imag = mean_absolute_error(actual_imag.flatten(), pred_imag.flatten())\n",
    "r2_imag = r2_score(actual_imag.flatten(), pred_imag.flatten())\n",
    "\n",
    "print(f\"Métricas para a parte real (CNN+LSTM+VAR): MSE: {mse_real:.6f}, MAE: {mae_real:.6f}, R²: {r2_real:.6f}\")\n",
    "print(f\"Métricas para a parte imaginária (CNN+LSTM+VAR): MSE: {mse_imag:.6f}, MAE: {mae_imag:.6f}, R²: {r2_imag:.6f}\")\n",
    "\n",
    "# Função para calcular NMSE por coeficiente\n",
    "def calculate_normalized_mse(actual, predicted):\n",
    "    num_coefs = actual.shape[1]\n",
    "    nmse = np.zeros(num_coefs)\n",
    "    for c in range(num_coefs):\n",
    "        mse = mean_squared_error(actual[:, c], predicted[:, c])\n",
    "        variance = np.var(actual[:, c])\n",
    "        # Evitar divisão por zero\n",
    "        if variance != 0:\n",
    "            nmse[c] = mse / variance\n",
    "        else:\n",
    "            nmse[c] = np.nan\n",
    "    return nmse\n",
    "\n",
    "# Calcular NMSE para a parte real\n",
    "nmse_real = calculate_normalized_mse(actual_real, pred_real)\n",
    "\n",
    "# Calcular NMSE para a parte imaginária\n",
    "nmse_imag = calculate_normalized_mse(actual_imag, pred_imag)\n",
    "\n",
    "# Calcular NMSE para o módulo dos coeficientes\n",
    "actual_modulus = np.sqrt(actual_real**2 + actual_imag**2)\n",
    "predicted_modulus = np.sqrt(pred_real**2 + pred_imag**2)\n",
    "nmse_modulus = calculate_normalized_mse(actual_modulus, predicted_modulus)\n",
    "\n",
    "# Calcular a média do NMSE sobre todos os coeficientes\n",
    "average_nmse_real = np.nanmean(nmse_real)\n",
    "average_nmse_imag = np.nanmean(nmse_imag)\n",
    "average_nmse_modulus = np.nanmean(nmse_modulus)\n",
    "\n",
    "print(f\"Média do NMSE para a parte real (CNN+LSTM+VAR): {average_nmse_real:.6f}\")\n",
    "print(f\"Média do NMSE para a parte imaginária (CNN+LSTM+VAR): {average_nmse_imag:.6f}\")\n",
    "print(f\"Média do NMSE para o módulo dos coeficientes (CNN+LSTM+VAR): {average_nmse_modulus:.6f}\")\n",
    "\n",
    "# Plotar NMSE do Módulo por coeficiente\n",
    "coefficients = np.arange(1, num_coefs + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(coefficients, nmse_modulus, width=0.6, label='Módulo (CNN+LSTM+VAR)', color='green')\n",
    "plt.xlabel('Índice do Coeficiente')\n",
    "plt.ylabel('NMSE')\n",
    "plt.title('NMSE por Coeficiente (Módulo) - CNN+LSTM+VAR')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(output_dir, 'nmse_por_coeficiente_modulo_cnn_lstm_var.png'), dpi=400)\n",
    "plt.close()\n",
    "\n",
    "# Plotar a função de perda (loss) ao longo das épocas\n",
    "if retrain_model:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history.history['loss'], label='Perda de Treinamento')\n",
    "    plt.plot(history.history['val_loss'], label='Perda de Validação')\n",
    "    plt.title('Função de Perda ao Longo das Épocas - CNN+LSTM+VAR')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Perda (MSE)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(output_dir, 'funcao_perda_cnn_lstm_var.png'), dpi=400)\n",
    "    plt.close()\n",
    "\n",
    "# Plotar coeficientes aleatórios em subplots\n",
    "num_plots = 4  # Número de coeficientes a serem plotados\n",
    "random_indices = np.random.choice(num_coefs, num_plots, replace=False)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for idx, coef_index in enumerate(random_indices):\n",
    "    plt.subplot(2, 2, idx + 1)\n",
    "    plt.plot(actual_real[:, coef_index], label=f'Parte Real - Coeficiente {coef_index+1} (Real)')\n",
    "    plt.plot(pred_real[:, coef_index], label=f'Parte Real - Coeficiente {coef_index+1} (Predito)', linestyle='--')\n",
    "    plt.title(f'Comparação da Parte Real do Coeficiente {coef_index+1} (Validação) - CNN+LSTM+VAR')\n",
    "    plt.xlabel('Amostra de Tempo')\n",
    "    plt.ylabel('Valor')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'comparacao_parte_real_coeficientes_cnn_lstm_var.png'), dpi=400)\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for idx, coef_index in enumerate(random_indices):\n",
    "    plt.subplot(2, 2, idx + 1)\n",
    "    plt.plot(actual_imag[:, coef_index], label=f'Parte Imaginária - Coeficiente {coef_index+1} (Real)')\n",
    "    plt.plot(pred_imag[:, coef_index], label=f'Parte Imaginária - Coeficiente {coef_index+1} (Predito)', linestyle='--')\n",
    "    plt.title(f'Comparação da Parte Imaginária do Coeficiente {coef_index+1} (Validação) - CNN+LSTM+VAR')\n",
    "    plt.xlabel('Amostra de Tempo')\n",
    "    plt.ylabel('Valor')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'comparacao_parte_imaginaria_coeficientes_cnn_lstm_var.png'), dpi=400)\n",
    "plt.close()\n",
    "\n",
    "# Adicionar a plotagem da evolução do NMSE ao longo do tempo\n",
    "nmse_time = np.mean((y_val_original - cnn_lstm_predictions)**2, axis=1) / np.var(y_val_original, axis=0).mean()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(nmse_time, label='NMSE ao Longo do Tempo')\n",
    "plt.xlabel('Amostra de Tempo')\n",
    "plt.ylabel('NMSE')\n",
    "plt.title('Evolução do NMSE ao Longo das Amostras de Tempo - CNN + LSTM + VAR')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(output_dir, 'nmse_tempo_cnn_lstm.png'), dpi=400)\n",
    "plt.close()\n",
    "\n",
    "# Plotar a comparação dos perfis de velocidade 'v' e 'w' originais e reconstruídos\n",
    "\n",
    "# Exemplo para 'v'\n",
    "plt.figure(figsize=(16, 12))\n",
    "plt.clf()\n",
    "for i in range(step):\n",
    "    plt.subplot(4, 5, i + 1)\n",
    "    idx = i * (X_rec_array.shape[1] // step)\n",
    "    plt.plot(np.abs(X_actual_array[ny:2*ny, idx]), y_positions, label='Original')\n",
    "    plt.plot(np.abs(X_rec_array[ny:2*ny, idx]), y_positions, label='Reconstruído', linestyle='--')\n",
    "    plt.xlabel('|v|')\n",
    "    plt.ylabel('y')\n",
    "    plt.title(f'Amostra {idx}')\n",
    "    plt.grid(True)\n",
    "    if i == 0:\n",
    "        plt.legend()\n",
    "plt.suptitle('Comparação de |v| Original e Reconstruído (Validação) - CNN + LSTM + VAR')\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.savefig(os.path.join(output_dir, 'comparacao_v_original_reconstruido_cnn_lstm_var.png'), dpi=400)\n",
    "plt.close()\n",
    "\n",
    "# Exemplo para 'w'\n",
    "plt.figure(figsize=(16, 12))\n",
    "plt.clf()\n",
    "for i in range(step):\n",
    "    plt.subplot(4, 5, i + 1)\n",
    "    idx = i * (X_rec_array.shape[1] // step)\n",
    "    plt.plot(np.abs(X_actual_array[2*ny:, idx]), y_positions, label='Original')\n",
    "    plt.plot(np.abs(X_rec_array[2*ny:, idx]), y_positions, label='Reconstruído', linestyle='--')\n",
    "    plt.xlabel('|w|')\n",
    "    plt.ylabel('y')\n",
    "    plt.title(f'Amostra {idx}')\n",
    "    plt.grid(True)\n",
    "    if i == 0:\n",
    "        plt.legend()\n",
    "plt.suptitle('Comparação de |w| Original e Reconstruído (Validação) - CNN + LSTM + VAR')\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.savefig(os.path.join(output_dir, 'comparacao_w_original_reconstruido_cnn_lstm_var.png'), dpi=400)\n",
    "plt.close()\n",
    "\n",
    "# Plotar NMSE para a parte real, imaginária e módulo dos coeficientes\n",
    "coefficients = np.arange(1, num_coefs + 1)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# NMSE da Parte Real\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.bar(coefficients, nmse_real, width=0.6, color='blue')\n",
    "plt.xlabel('Índice do Coeficiente')\n",
    "plt.ylabel('NMSE')\n",
    "plt.title('NMSE por Coeficiente - Parte Real (CNN+LSTM+VAR)')\n",
    "plt.grid(True)\n",
    "\n",
    "# NMSE da Parte Imaginária\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.bar(coefficients, nmse_imag, width=0.6, color='green')\n",
    "plt.xlabel('Índice do Coeficiente')\n",
    "plt.ylabel('NMSE')\n",
    "plt.title('NMSE por Coeficiente - Parte Imaginária (CNN+LSTM+VAR)')\n",
    "plt.grid(True)\n",
    "\n",
    "# NMSE do Módulo\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.bar(coefficients, nmse_modulus, width=0.6, color='orange')\n",
    "plt.xlabel('Índice do Coeficiente')\n",
    "plt.ylabel('NMSE')\n",
    "plt.title('NMSE por Coeficiente - Módulo (CNN+LSTM+VAR)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'nmse_por_coeficiente_todas_cnn_lstm_var.png'), dpi=400)\n",
    "plt.close()\n",
    "\n",
    "# Exibir o tempo total de execução do script\n",
    "total_end_time = time.time()\n",
    "print(f\"Tempo total de execução do script: {total_end_time - total_start_time:.2f} segundos\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
